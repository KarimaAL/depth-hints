# -*- coding: utf-8 -*-
"""Copie de not for commercial purposes.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/KarimaAL/depth-hints/blob/master/Copie_de_not_for_commercial_purposes.ipynb
"""


#.........................................Explaining the project step by step:..................................

# Commented out IPython magic to ensure Python compatibility.
from __future__ import absolute_import, division, print_function
# %matplotlib inline
import skimage.io
import os
import numpy as np
import PIL.Image as pil
import matplotlib.pyplot as plt

import torch
from torchvision import transforms
from google.colab.patches import cv2_imshow 
import networks
from utils import download_model_if_doesnt_exist
import re 
import cv2
import glob
from PIL import Image

import re 
import cv2
import glob
from PIL import Image

model_name = "mono_640x192"

download_model_if_doesnt_exist(model_name)
encoder_path = os.path.join("models", model_name, "encoder.pth")
depth_decoder_path = os.path.join("models", model_name, "depth.pth")

# LOADING PRETRAINED MODEL
encoder = networks.ResnetEncoder(18, False)
depth_decoder = networks.DepthDecoder(num_ch_enc=encoder.num_ch_enc, scales=range(4))

loaded_dict_enc = torch.load(encoder_path, map_location='cpu')
filtered_dict_enc = {k: v for k, v in loaded_dict_enc.items() if k in encoder.state_dict()}
encoder.load_state_dict(filtered_dict_enc)

loaded_dict = torch.load(depth_decoder_path, map_location='cpu')
depth_decoder.load_state_dict(loaded_dict)

encoder.eval()
depth_decoder.eval();

img_dir = "/content/drive/My Drive/VIDEO PEDESTRIAN" # Enter Directory of all images 

data_path = os.path.join(img_dir,'*g')
files = glob.glob(data_path)

dirFiles = files
dirFiles.sort( key=lambda f: int(re.sub('\D', '', f)),)
print(dirFiles)
data = []
pic_num = 1
for f1 in dirFiles:
    img = cv2.imread(f1)
        
    # input_image = Image.open(f1).convert('RGB')
    # original_width, original_height = input_image.size
    input_image1 = Image.open(f1).convert('RGB')
    original_width, original_height = input_image1.size
    input_image     = input_image1.crop((1,152,1280,800)) ## left, up, right, bottom
    original_width, original_height = input_image.size
    feed_height = loaded_dict_enc['height']
    feed_width = loaded_dict_enc['width']

    print(input_image.size)

    input_image_resized = input_image.resize((feed_width, feed_height), pil.LANCZOS)

    input_image_pytorch = transforms.ToTensor()(input_image_resized).unsqueeze(0)

        data.append(img)
    with torch.no_grad():
      features = encoder(input_image_pytorch)
      outputs = depth_decoder(features)

    disp = outputs[("disp", 0)]

    disp_resized = torch.nn.functional.interpolate(disp,
    (original_height, original_width), mode="bilinear", align_corners=False)

    # Saving colormapped depth image
    disp_resized_np = disp_resized.squeeze().cpu().numpy()
    vmax = np.percentile(disp_resized_np, 95)
    
    plt.figure(figsize=(10, 10))
    plt.subplot(211)
    plt.imshow(input_image)
    plt.title("Input", fontsize=22)
    plt.axis('off')

    plt.subplot(212)
    plt.imshow(disp_resized_np, cmap='magma', vmax=vmax)
    plt.title("Disparity prediction", fontsize=22)
    plt.axis('off');


    skimage.io.imsave('/content/another window/image' + str(pic_num) + '.png',disp_resized_np)#cv2.cvtColor(np.float32(disp_resized_np), cv2.COLOR_RGB2BGR))#COLOR_GRAY2BGR))

  
    imageTest=cv2.imread('/content/another window/image' + str(pic_num) + '.png')
    imageTest = cv2.applyColorMap(cv2.convertScaleAbs(imageTest, alpha=1.8), cv2.COLORMAP_MAGMA)
    #imageTest = cv2.applyColorMap(cv2.convertScaleAbs(imageTest, alpha=1), cv2.COLORMAP_JET)
    # imageTest = cv2.applyColorMap(cv2.convertScaleAbs(imageTest, alpha=1), cv2.COLORMAP_HSV)
    #imageTest = cv2.applyColorMap(cv2.convertScaleAbs(imageTest, alpha=1.8), cv2.COLORMAP_PLASMA)
    #imageTest = cv2.applyColorMap(cv2.convertScaleAbs(imageTest, alpha=0.9), cv2.COLORMAP_RAINBOW)
    #imageTest = cv2.applyColorMap(cv2.convertScaleAbs(imageTest, alpha=1.8), cv2.COLORMAP_TURBO)
    
    plt.imshow(imageTest)
    plt.show

  
    input_image=cv2.cvtColor(np.float32(input_image),cv2.COLOR_RGB2BGR)
    concat = np.hstack((input_image, imageTest))
    cv2.imwrite('/content/concatenated/image' + str(pic_num) + '.png',concat)#cv2.cvtColor(np.float32(concat),cv2.COLOR_RGB2BGR))

    #cv2_imshow(concat)
    from google.colab import files
    files.download('/content/concatenated/image' + str(pic_num) + '.png')
    pic_num = pic_num + 1

